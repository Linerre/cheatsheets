% \section*{Threads Advantages}
% \begin{enumerate}
% \item \textbf{Software Portability} Threaded applications can be developed on serial machines and run on parallel machines without any changes.
% \item \textbf{Latency Hiding} In effect, while one thread is waiting for a comm op, other threads can utilize CPU, thus masking associated overhead
% \item \textbf{Scheduling \& Load Balancing} Threaded APIs allow programmers to specify a large number of concurrent tasks and support system-level dynamic mapping of tasks to processors with a view to minimizing idling overheads,  ridding the programmer of the burden of explicit scheduling and load balancing
% \item \textbf{Ease of Programming, Widespread Use} 1-3 make threaded APIs significantly easier to use than message passing APIs. Achieving identical performance for the two patterns may require additional effort
% \end{enumerate}
\section*{PThreads (for sys (not app) programmers)}
\begin{minipage}{0.5\linewidth}
  \flushleft
  \begin{itemize}
  \item On successful creation, a unique identifier is associated with the created thread
  \item \texttt{t\_handle} var is written before the \texttt{pthread\_create} returns
  \item Use \texttt{struct} to pass multiple \texttt{arg}s or to return multiple vals
  \item New thread is ready for execution as soon as it is created
  \item On the same processor, new thread may preempt its creator
  \end{itemize}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt,xrightmargin=2pt]
#include <pthread.h>
int // 0: Ok | non-zero: Err
pthread_create (
  // points to created thread
  pthread_t *t_handle,
  // if NULL, default attrs
  const pthread_attr_t *attr,
  // thread runs this fn
  void * (*t_fn)(void *),
  // to pass data to t_fn
  // NULL means no data
  void *arg);
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \flushleft
  \begin{itemize}
  \item \textbf{implicit} termination: when top-level thread returns
  \item \texttt{exit} explicitly terminates main thread and will wait for all other peers to end
  \item peer thread calls \texttt{exit} to end itself and all its child threads
  \item \texttt{cancel} sends request only (async)
  \item Threads can protect themselves against cancellation
  \item 0 only means target thread is valid for cancellation
  \end{itemize}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt,xrightmargin=2pt]
#include <pthread.h>
// get tid for creaded thread
pthread_t pthread_self(void);

// explicit termination
// 0 (Ok) | non-zero (Err)
void pthread_exit(
  void *thread_return);

// 0 (Ok) | non-zero (Err)
int pthread_cancel(
  pthread_t thread);
\end{lstlisting}
\end{minipage}

\begin{minipage}{0.5\linewidth}
  \flushleft
  \begin{itemize}[label=\ding{169}]
  \item current thread waits (also blocks) until thread \texttt{tid} ends
  \item thread \texttt{tid} returns \texttt{void *} and this pointer gets assigned to location pointed by \texttt{thread\_return}
  \item Reaps memory resrc (e.g. stack) of terminated thread: managing/reclaiming memory no longer needed by a program
  \end{itemize}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \flushleft
  \begin{itemize}[label=\ding{169}]
  \item thread is either \emph{joinable} or \emph{detachable}
  \item joinable keeps its mem resrc (stack) until it's reaped
  \item joinable should be explicity reaped by others or detached by \texttt{detach()} (to avoid mem leaks)
  \item detachable cannot be reaped by others and its mem resrc freed automatically by sys when it ends
  \end{itemize}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xrightmargin=2pt]
#include <pthread.h>
// 0 (Ok) | non-zero (Err)
pthread_t pthread_join(
  pthread_t tid,
  void **thread_return
);
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=4pt,xrightmargin=2pt]
#include <pthread.h>
// 0 (Ok) | non-zero (Err)
int pthread_detach(
  pthread_t tid);
// if tid == thread_self
// then detach itself
\end{lstlisting}
\end{minipage}
\subsection*{Sync thread, critical section and mutex}
% Init
\begin{minipage}{0.5\linewidth}
  \flushleft
  This fn init the \texttt{mutex\_lock} to an unlocked state and with specified \texttt{lock\_attr}. If this argument is set to \texttt{NULL}, it's a normal mutex-lock with default attrs.
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt]
#include <pthread.h>
int pthread_mutex_init (
 pthread_mutex_t *mutex_lock,
 const pthread_mutexattr_t *lock_attr);
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.5\linewidth}
  \flushleft
  If already locked, calling thread blocks; otherwise the mutex-lock is locked and the calling thread returns 0 (Ok).   Other values indicate errors such as deadlocks.
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt]
#include <pthread.h>
// attempt a lock on mutex_lock
int pthread_mutex_lock (
  pthread_mutex_t *mutex_lock
); // consume CPU cycles
\end{lstlisting}
\end{minipage}
% unlock
\begin{minipage}{0.5\linewidth}
  \flushleft
  If no unlocking, no other thread can enter this section $\rightarrow$ deadlock.  Unlocking previously unlocked or that already locked by another thread is undefined effect.  In case of normal mutex-lock, it's released and one of the blocked threads is scheduled to enter critical sec.
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt]
#include <pthread.h>
// on leaving a critical section
// must unlock section mutex_lock
int pthread_mutex_unlock (
  pthread_mutex_t *mutex_lock
);
// schedule policy decides which
// thread will get the lock next
\end{lstlisting}
\end{minipage}
\subsection*{Locking overheads and alleviation}
Locks represent serial parts since critical sections must be executed by threads one after the other.  Large segments within locks can lead to significant performance degradation. It is important to \textbf{minimize} the size of critical sections. If time for a lock-update-count-unlock cycle is $t_1$ and time to find an entry is $t_2$, then total time for query is $(t_1+t_2)\times n_{max}$, where $n_{max}$ is maximum number of entries found by any thread. If $t_1$ and $t_2$ are comparable, then locking leads to considerable overhead
\begin{lstlisting}[language=C,xleftmargin=0pt,xrightmargin=0pt]
int output_record(struct database_record *record_ptr) {
    int count;
    pthread_mutex_lock(&output_count_lock);   // lock
    output_count ++;
    count = output_count;
    pthread_mutex_unlock(&output_count_lock); // unlock
    if (count <= requested_number_of_records)
        print_record(record_ptr);
    return (count);
}
\end{lstlisting}
\begin{minipage}{0.5\linewidth}
  \flushleft
  \begin{itemize}
  \item reduce the idling overhead associated with locks
  \item allows thread to do other work and to poll the mutex for a lock
  \item typically much faster
  \end{itemize}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=0pt,xrightmargin=2pt]
// 0     -> Ok, get the lock
// EBUSY -> lock unavailble
int pthread_mutex_trylock (
  pthread_mutex_t *mutex_lock
);
\end{lstlisting}
\end{minipage}
\begin{lstlisting}[language=C,xleftmargin=0pt,xrightmargin=0pt]
int output_record(struct database_record *record_ptr) {
    int count;
    int lock_status;
    // periodically poll for lock avail. (still introduce overhead)
    lock_status = pthread_mutex_trylock(&output_count_lock);
    if (lock_status == EBUSY) {
        insert_into_local_list(record_ptr);
        return(0);
    }
    else {
        count = output_count;
        output_count += number_on_local_list + 1; // 1 for curr record
        pthread_mutex_unlock(&output_count_lock);
        print_records(record_ptr, local_list,
              requested_number_of_records - count);
        return(count + number_on_local_list + 1);
    }
}
\end{lstlisting}
\subsection*{Producer-consumer work queues}
\begin{lstlisting}[language=C,framexbottommargin=2pt]
#include <pthread.h>
pthread_mutex_t task_queue_lock;
int task_available;
task_available = 0;                              // init cond flag
pthread_init();                                  // normal init thread
pthread_mutex_init(&task_queue_lock, NULL); // normal init lock
void *producer(void *producer_thread_data) {
    int inserted;
    struct task my_task;
    while (!done()) {
        inserted = 0;
        create_task(&my_task);
        while (inserted == 0) { // if not inserted task into queue
            pthread_mutex_lock(&task_queue_lock); // lock task queue
            if (task_available == 0) {
                insert_into_queue(my_task);           // insert one task
                task_available = 1;                   // for consumer
                inserted = 1;                         // set the cond flag
            }                                         // unlock task queue
            pthread_mutex_unlock(&task_queue_lock);
}}}
\end{lstlisting}
\begin{lstlisting}[language=C]
void *consumer(void *consumer_thread_data) {
    int extracted;
    struct task my_task;
    /* local data structure declarations */
    while (!done()) {
        extracted = 0;
        while (extracted == 0) { // if not extracted a task
            pthread_mutex_lock(&task_queue_lock);  // lock the task queue
            if (task_available == 1) {
                extract_from_queue(&my_task);         // extract one task
                task_available = 0;                   // for producer
                extracted = 1;                        // set the cond flag
            }                                         // unlock task queue
            pthread_mutex_unlock(&task_queue_lock);
        }
        process_task(my_task);}}

/*--- conditional varaible always has a mutex asscoiated with it ---*/
pthread_cond_t cond_queue_empty, cond_queue_full;  // two cond vars
pthread_mutex_t task_queue_cond_lock; // lock assoc. with cond vars
int task_available;                  // predicate assoc. with cond vars
task_available = 0;
pthread_init();
pthread_cond_init(&cond_queue_empty, NULL);  // NULL means default attrs
pthread_cond_init(&cond_queue_full, NULL);
pthread_mutex_init(&task_queue_cond_lock, NULL);

void *producer(void *producer_thread_data) {
    int inserted;
    while (!done()) {
        create_task();
        pthread_mutex_lock(&task_queue_cond_lock); // lock
        while (task_available == 1) // test predicate (shared var)
            // thread gets blocked until recv a signal from other or OS
            pthread_cond_wait(&cond_queue_empty, &task_queue_cond_lock);
        insert_into_queue(); // if predicate false, insert one task
        task_available = 1;  // set predicate to true
        pthread_cond_signal(&cond_queue_full);        // signal consumer
        pthread_mutex_unlock(&task_queue_cond_lock);  // unlock
    }}
/* each task consumed by only one consumer thread */
void *consumer(void *consumer_thread_data) {
    while (!done()) {
        pthread_mutex_lock(&task_queue_cond_lock); // lock
        while (task_available == 0) // test predicate
            // wait on cond_queue_full (not consume CPU cycles)
            pthread_cond_wait(&cond_queue_full, &task_queue_cond_lock);
        my_task = extract_from_queue();  // extract one task (pred. true)
        task_available = 0; // set predicate to false
        pthread_cond_signal(&cond_queue_empty);       // signal producer
        pthread_mutex_unlock(&task_queue_cond_lock);  // unlock
        process_task(my_task); }}
\end{lstlisting}
% cond wait
\begin{minipage}{0.5\linewidth}
  \flushleft
  \texttt{*\_wait} fns \textbf{release} the lock for other thread to work on shared var; reacquire the lock before resuming
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt,xrightmargin=2pt]
int pthread_cond_wait(
   pthread_cond_t *cond,
   pthread_mutex_t *mutex);
\end{lstlisting}
\end{minipage}
% signal
\begin{minipage}{0.5\linewidth}
  \flushleft
  indicate shared var change and wake at least 1 waiting thread
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=1pt,xrightmargin=2pt]
int pthread_cond_signal(pthread_cond_t *cond);
\end{lstlisting}
% init / destroy
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xrightmargin=2pt]
// discard unneeded cond vars
int pthread_cond_destroy(
   pthread_cond_t *cond);
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\begin{lstlisting}[language=C,xleftmargin=2pt]
int pthread_cond_init(
   pthread_cond_t *cond,
   const pthread_condattr_t *attr);
\end{lstlisting}
\end{minipage}
\begin{lstlisting}[language=C]
int pthread_cond_broadcast(pthread_cond_t *cond);//wake all the waiting
int pthread_cond_timedwait(pthread_cond_t *cond,
   pthread_mutex_t *mutex,             // wake up itself after waiting for
   const struct timespec *abstime); // this timeout
\end{lstlisting}
