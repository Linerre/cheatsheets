\begin{description}
\item[address space] (of a process) contains all of the memory state of the
running program: the \textbf{code} of the program (the instructions); The running program uses a \textbf{stack} to keep track of where it is in function call chain as well as to allocate local variables and pass parameters and return values to and from routines; the \textbf{heap} is used for dynamically-allocated, user-managed memory, etc.  It is the running program’s view of memory in the system

\item[limited direct execution] let the program run directly on the hardware; however, at certain key points in time (such as when a process issues a system call, or a timer interrupt occurs), arrange so that the OS gets involved and makes sure the “right” thing happens.   Thus, the OS, with a little hardware support, tries best to get out of the way of the running program, to deliver an \emph{efficient} virtualization.

\item[LRU] least recently used: take advantage of locality in the memory-reference stream, assuming it is likely that an entry that has not recently been used is a good
candidate for eviction.

\item[most-recently-used (MRU)] MostFrequently-Used (MFU) and Most-Recently-Used (MRU). In most cases (not all!), these policies do not work well, as they ignore the locality most programs exhibit instead of embracing it.

\item[memory virtualization] OS maps user program address space to physical memory address.

\item[memory management unit] the part of the processor that helps with address translation (base and bounds registers kept on the chip).

\item[paging] Chop memory into \emph{fixed-sized} pieces, each called a \textbf{page}

\item[page frame] physical memory viewed as an array of fixed-sized slots called page frames; each of these frames can contain a single virtual-memory page.

\item[page table] s a per-process data structure maintained by OS.  Its major role is to store address translations for each of the virtual pages of the address space, thus letting us know where in physical memory each page resides.

\item[page fault] the act of accessing a page that is not in physical memory.  Upon a page fault, the OS is invoked to service the page fault.  Virtually all systems handle page faults in software.

\item[segmentation] Chop memory into \textsw{variable-sized} chunks and allocate them based on needs (e.g in base/bound or segmentation approaches)

\item[segmentation fault] a violation or error arising from a memory access on a segmented machine to an illegal address.

\item[sparse address space] large address spaces with large amounts of unused address space

\item[translation-lookaside buffer (TLB)] part of the chip’s memory-management unit (MMU), and is simply a hardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache.

\item[time-space-trade-offs] Usually, if you wish to make access to a particular data structure faster, you will have to pay a space-usage penalty for the structure.

\item[virtual address] All that in user program address space



\item[peak flops/sec] $ = \# \text{cores} \times [\# \text{sockets}] \times \# \text{flops} \times \text{freq}$

\item[speedup (hard due to overheads)]
  \begin{enumerate*}
  \item idling (unbalanced load, sync, serial parts, etc)
  \item splitting computation into tasks
  \item communications among processes
  \end{enumerate*}

\item[speedup (fixed problem size) and efficiency] $S_p = \frac{T_{seq}}{T_{par}} (\geq 1)\qquad E_p = \frac{S_{p}}{p} (0 < E_p \leq 1)$

% \item[efficiency] $E_p = \frac{S_{p}}{p} (0 < E_p \leq 1)$

\item[embarrassingly (easy) parallel] aka. perfectly parallel, delightfully parallel or pleasingly parallel, where problem can be easily divided into independent parts and solved without communication

\item[strong scalability] fixed problem size + increasing $\# p \rightarrow$ perf $\downarrow$

\item[weak scalability] increasing $\# p$ and problem size $\rightarrow$ perf $\downarrow$

\item[communication latency] time taken to communicate a msg between 2 procs in a network

\item[minimal routing] takes 1 of shortest paths (XY-routing; E-cube)

\item[non-minimal routing] route the message along a longer path to avoid network congestion

\item[deterministic] routing determines a unique path \emph{solely} based on src and dest nodes

\item[adaptive] routing uses info on network state to determine message path

\item[walltime] the actual time taken from the start of a computer program to the end. aka. elapsed real time, real time, wall-clock time
\item[flops] floating-point operations (add, subtract, multiply, divide) per second
\item[ceil] ceil(n,p) = $(n + p - 1) / p$ especially when $n$ isn't evenly divisible by $p$

\item[synchronous] 2 procs A, B. A will only start sending when B explicitly signals it is ready to receive.  If not, A just waits (also blocks).  In such mode, two procs communicate with each other directly.

\item[asynchronous] All concurrent tasks execute asynchronously; possible to implement any parallel algorithm.  However, it is hard to reason about the program because of non-deterministic behavior due to race conditions.

\item[loosely synchronous] a good compromise between aysnc and sync.  Tasks and subtasks synchronize to perform interactions (easy to debug), while between interactions, tasks execute completely asynchronously.

\item[blocking, non-buffered] A sends and B recvs, no system buffer available.  A must hold the data (A being idle and blocking) until B is ready to receive.

\item[blocking, buffered] data/message can be copied into a system-controlled block of memory (buffer on A or B or both) so A can continue to execute.  When B is ready, it copies the buffered data into its own appropriate memory.  A \textbf{common} implementation is to buffer relatively small messages and switch to non-buffered mode for large messages (blocking too).  In general, better to write programs with bounded buffer requirements.  This blocking time is only the period of copying data into/from buffer.
\item[non-blocking] generally accompanied by a \texttt{check-status} operation.  Upon return from non-blocking send/recv, the proc is free to perform any computation that doesn't require communication.  Later the proc \textbf{checks} if non-blocking op has completed and wait for its completion if needed.
\item[communication domain] a set of processes allowed to communicate with each other.  Info about this domain is stored in vars of type \texttt{MPI\_Comm} aka \textbf{communicators} (used as arg to all message transfer MPI routines and they uniquely identify procs). One proc can belong to many different (possibly overlapping) communication domains.  \texttt{MPI\_COMM\_WORLD} includes all procs in parallel execution as they all may need to talk to each other.

\item[one-to-all broadcast] a single process sends \textbf{identical} data to all other processes or to a subset of them.  Initially, only the source process has the data of size $m$ that needs to be broadcast.  At the termination of the procedure, there are $p$ copies of the initial data --- one belonging to each process.

\item[all-to-one reduction] each of the p participating processes starts with a buffer $M$ containing $m$ words.  The data from all processes are combined through an associative operator and accumulated at a single destination process into one buffer of size $m$.  Reduction can be used to find the sum, product, maximum, or minimum of sets of numbers

\item[latency] At the logical level, a memory system (of multiple levels of caches) takes in a request for a memory word and returns a block of data of size $b$ containing the requested word after $l$ nanoseconds, referred to as the \textbf{latency} of the memory.  As an analogy, if water comes out of the end of a fire hose 2 seconds after a hydrant is turned on, then the latency of the system is 2 seconds.


\item[bandwidth] The rate at which data can be pumped from the memory to the processor determines the bandwidth of the memory system.  How much data (per unit time) gets transferred from memory to processor.


\item[hit ration] The fraction of data references satisfied by the cache is called the cache hit ratio of the computation on the system.   The effective computation rate of many applications is bounded \emph{not} by the processing rate of the CPU, but by the rate at which data can be pumped into the CPU.  Such computations are referred to as being \textbf{memory bound}.


\item[temporal locality] If at one point a particular mem location is ref-ed, then it's likely that the same location will be ref-ed again soon

\item[spatial locality] Data that is accessed or referenced spatially close to each other (\texttt{a[i-1],a[i],a[i+1]}) is likely to be accessed again in the near future

\item[recursive decomposition] a method for inducing concurrency in problems that can be solved using the divide-and-conquer strategy.  A problem is solved by first dividing it into a set of independent subproblems. Each subproblem is solved by recursively applying a similar division into smaller subproblems followed by a combination of their results. (quicksort)


\item[data decomposition] a powerful/commonly used method for deriving concurrency in algorithms that operate on large data structures.  The decomposition of computations is done in two steps.  First, the data on which the computations are performed is partitioned; second, this data partitioning is used to induce a partitioning of the computations into tasks. The operations these tasks perform on different data partitions are usually similar or are chosen from a small set of operations (LU factorization).


\item[partitioning output data] In many computations, each element of the output can be computed naturally independently of others as a function of the input. In such computations, a partitioning of the output data automatically induces a decomposition of the problems into tasks, where each task is assigned the work of computing a portion of the output. (matrix-multiplication)


\item[partioning input data] When impossible to split output data, it is possible to partition the input data, and then use this partitioning to induce concurrency. A task is created for each partition of the input data and this task performs as much computation as possible using these local data. Note that the solutions to tasks induced by input partitions may not directly solve the original problem. In such cases, a follow-up computation is needed to combine the results. (sum of n elements)

\item[exploratory decomposition] is used to decompose problems whose underlying computations correspond to a search of a space for solutions. In exploratory decomposition, we partition the search space into smaller parts, and search each one of these parts concurrently, until the desired solutions are found. For an example of exploratory decomposition, consider the 15-puzzle problem.







\end{description}
