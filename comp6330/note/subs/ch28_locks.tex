\section*{(most) Locks}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c,xrightmargin=2pt]
// globally-allocated lock 'mutex'
lock_t mutex; // declare lock var
// ... other work ...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \begin{itemize}
  \item must declare a lock before using it
  \item lock available = unlocked, free $\to$ no thread holds it
  \item lock acquired = locked, held $\to$ exactly one thread holds it (likely in a critical section)
  \end{itemize}
\end{minipage}
\begin{itemize}
\item other info like which thread holds lock or a queue for ordering lock acquisition can be stored in lock but often hidden from user
\item calling \texttt{lock()} will try to acquire lock (1. free; 2. not free)
  \begin{enumerate}
  \item thread acquires lock and enters critical section; \textbf{owner} of lock
  \item thread waits (not return) and cannot enter critical section
  \end{enumerate}
\item calling \texttt{unlock()} will free the lock (1. no waiting thread; 2. opposite)
  \begin{enumerate}
  \item lock becomes free and any thread can try to acquire it
  \item one of waiting threads will acquire lock and enters critical section
  \end{enumerate}

\item \textbf{coarse-grained}: one big lock for access to critical section at any time
\item \textbf{fine-grained}: different locks to protect different data and data structs (allowing more threads in different locked code at once)
\end{itemize}
\section*{Evaluating Locks}
\begin{enumerate}
\item \textbf{correctness} lock provides \textbf{mutual exclusion}
\item \textbf{fairness} each thread gets a fair shot at acquiring the lock (once it's free); no thread contending for the lock starve while doing so
\item \textbf{performance} time overhead added by using the lock:
  \begin{enumerate}
  \item single thread (no contention) $\to$ what is the overhead?
  \item multiple threads contending for lock on single CPU
  \item lock perf. on multiple CPUs and threads contending for the lock
  \end{enumerate}
\end{enumerate}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c,xrightmargin=2pt]
void lock() { // before entering
  DisableInterrupts();
}
void unlock() { // after leaving
  EnableInterrupts();
}
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \begin{itemize}
  \item one of earliest solution offers mutex by disabling interrupts
  \item code inside critical section (not interrupted) runs as if it were atomic
  \item invented for single-processor systems;  main advantage: simplicity
  \end{itemize}
\end{minipage}
\begin{enumerate}
\item \textbf{insecure} arbitrary thread will need to perform \emph{privileged} ops so greedy/malicious progs can monopolize CPU and exploit system
\item \textbf{importable} not working on multiprocessors: threads enter same critical section via other CPUs $\to$ null effect of disabling interrupts
\item \textbf{buggy} turning off interrupts for too long $\to$ lost useful/critical interrupts that make OS work as expected (e.g. disturb I/O awakening)
\end{enumerate}
Thus, limited use case: OS occasionally uses interrupt masking to guarantee atomicity when accessing its own data structures
\section*{Failed Attempt: Just Using Loads/Stores}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c,xrightmargin=2pt]
typedef struct __lock_t
{ int flag; } lock_t;

void init(lock_t *mutex) {
  // 0: lock is avail | 1: held
  mutex->flag = 0; }

void lock(lock_t *mutex) {
  while (mutex->flag == 1) // TEST
     ; // spin-wait (do nothing)
  mutex->flag = 1; // now SET it }

void unlock(lock_t *mutex) {
   mutex->flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \begin{itemize}
  \item use 1 var \texttt{flag} to indicate thread's ownership of lock
  \item when lock not free, other threads \textbf{spin-wait} in while loop
  \item once lock free, waiting thread gets lock $\to$ set flat to 1
  \item correctness problem: both of two threads can set flag to 1 (with interrupts) and enter same critical section $\to$ mutex \textbf{not} guaranteed
  \item performance issue: thread endless checks flag (spin-waiting) and wastes CPU cycles
  \end{itemize}
\end{minipage}
\section*{Working Spin Locks with Test-And-Set (atomic exchange)}
\begin{itemize}
\item test old value and simultaneously setting the mem location to new value
\item return the old value pointed by \texttt{old\_ptr} and set \texttt{new} at same time
\item this sequence of operations is performed \textbf{atomically}
\end{itemize}
\begin{minipage}{.7\linewidth}
\begin{lstlisting}[language=c]
int TestAndSet(int *old_ptr, int new) {
  int old = *old_ptr;// fetch old value at old_ptr
  *old_ptr = new;      // store new into old_ptr
  return old;          // return the old value
}

typedef struct __lock_t { int flag; } lock_t;

// 0: lock is avail, 1: lock is held
void init(lock_t *lock) { lock->flag = 0; }

void lock(lock_t *lock) {
   while (TestAndSet(&lock->flag, 1) == 1)
     ; // spin-wait (do nothing)
}

void unlock(lock_t *lock) { lock->flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.3\linewidth}
  \flushleft
  \begin{itemize}
  \item make both \textbf{test} (old lock value) and \textbf{set} (new value) a single atomic op $\to$ only one thread acquires the lock $\to$ working mutual exclusion primitive
  \item On single CPU, needs a preemptive scheduler (interrupt via timer); otherwise a thread spins on a CPU and never gives it up
  \end{itemize}
\end{minipage}
\begin{enumerate}
\item \textbf{correctness} yes, only a single thread enters critical section at a time
\item \textbf{fairness} not generally, thread may spin forever $\to$ starve/zero fairness
\item \textbf{performance} \textbf{a} on single CPU, pretty bad: if N threads contending for the lock, each thread spins for duration of time slice before being timer-preempted. \textbf{b} on multiple CPU, reasonably well if \# of threads $\approx$ \# of CPUs and critical section is short (wasting fewer CPU cycles)
\end{enumerate}

\section*{Compare-and-Swap (SPARC) or Compare-and-Exchange (x86)}
\begin{minipage}{.65\linewidth}
\begin{lstlisting}[language=c]
// only diff with set-and-test shown below
int CompareAndSwap(int *ptr,
                     int expected, int new)
{
    int original = *ptr;
    if (original == expected)
       *ptr = new;
    return original;
}
void lock(lock_t *lock) {
   while(CompareAndSwap(&lock->flag,0,1) == 1)
      ; // spin
} // more powerful than test-and-set
\end{lstlisting}
\end{minipage}
\begin{minipage}{.35\linewidth}
  \flushleft
  \begin{itemize}
  \item test if value at addr \texttt{ptr} == expected;
  \item if so, update mem location \texttt{ptr} to new.
  \item If not, do nothing.
  \item In either case, return orig val at that mem location, thus allowing the code calling compare-and-swap to know if it succeeded
  \item for lock-free struct
  \end{itemize}
\end{minipage}
a simple spin lock built with it \texttt{==} test-and-set spin lock analyzed above
\section*{Load-Linked + Store-Conditional (MIPS, Alpha, PowerPC, ARM)}
\begin{minipage}{.65\linewidth}
\begin{lstlisting}[language=c]
int LoadLinked(int *ptr) { return *ptr; }
// atomic operation
int StoreConditional(int *ptr, int new) {
  if (no update to *ptr
      since LoadLinked to this address) {
     *ptr = new;
     return 1; // success!
  } else { return 0; }  // failed to update
}
// Using LL/SC To Build A Lock
void lock(lock_t *lock) {
  while (1) {
    while (LoadLinked(&lock->flag) == 1)
       ; // spin until it's zero
    if (StoreConditional(&lock->flag, 1) == 1)
       return;
      // if set-it-to-1 is a success: all done
      // otherwise: try it all over again
    }
}
void unlock(lock_t *lock) { lock->flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.35\linewidth}
  \flushleft
  \begin{itemize}
  \item load-linked operation likes load ix $\to$ fetch a value from addr \texttt{ptr} and place in a register
  \item store-cond succeeds (and updates the val) \emph{only} if no intervening store to the addr has taken place
  \item if succeeded, returns 1; val at \texttt{ptr} $\to$ \texttt{new}
  \item if failed, returns 0; val at \texttt{ptr} \emph{not} updated
  \item suppose 2 threads both exec \texttt{LoadLinked} and proceed to store-cond
  \item at this point only 1 thread will succeed in updating flag to 1
  \end{itemize}
\end{minipage}
\begin{minipage}{.45\linewidth}
\begin{lstlisting}[language=c]
int FetchAndAdd(int *ptr) {
   int old = *ptr;
   *ptr = old + 1;
   return old; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.55\linewidth}
  \begin{itemize}
  \item atomically increments a val while returning old val at particular addr
  \item In Intel x86, implemented using the ADD instruction with a prefix LOCK
  \end{itemize}
\end{minipage}
\section*{Ticket lock using Fetch-and-Add (ensures progress for all threads)}
\begin{minipage}{.55\linewidth}
\begin{lstlisting}[language=c]
typedef struct __lock_t {
  int ticket;
  int turn;
} lock_t;
void lock_init(lock_t *lock) {
  lock->ticket = 0;
  lock->turn = 0;
}
void lock(lock_t *lock) {
  int myturn;
  myturn = FetchAndAdd(&lock->ticket);
  while (lock->turn != myturn)
     ; // spin
}
void unlock(lock_t *lock) {
   lock->turn = lock->turn + 1;
}
\end{lstlisting}
\end{minipage}
\begin{minipage}{.45\linewidth}
  \flushleft
  \begin{itemize}
  \item when a thread tries to acquire a lock, it first does an \textbf{atomic} fetch-and-add on the ticket
  \item that value is now this thread's ``turn'' (\texttt{myturn})
  \item then use globally shared \texttt{lock->turn} to determine which thread's turn
  \item a thread enters critical section if \texttt{myturn == turn}
  \item unlocking increments the turn so next waiting thread (if any) can enter critical section
  \item thread with ticket will be scheduled later on
  \end{itemize}
\end{minipage}
\section*{Yield (good with 2 threads but worse with more)}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c]
void init() { flag = 0; }
void lock() {
  while (TestAndSet(&flag, 1) == 1)
    yield(); // give up the CPU
}
void unlock() { flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \flushleft
  \begin{itemize}
  \item assume OS with \texttt{yeild()} (give up CPU and let others run)
  \item thread may be running, ready, or blocked; \texttt{yeild}: running $\to$ ready
  \item yielding thd \textbf{deschedules} itself
  \item works well with 2 thds (no spin)
  \end{itemize}
\end{minipage}
\begin{itemize}
\item suppose there are 100 threads contending for a lock repeatedly
\item if one thread acquires lock but gets preempted before releasing it, the other 99 will each call \texttt{lock()}, find lock held, and yield CPU
\item if RR sched, each of 99 run-and-yeild before lock-holder runs again
\item no 99 time slice spinning but cost of ctx switch is substantial (waste)
\item this approach does \emph{not} stress starvation: A thread may get
caught in endless yield loop while others repeatedly enter/exit critical section
\end{itemize}
\section*{Lock with queues (correct, reasonably fair, limited spinning)}
\begin{minipage}{.6\linewidth}
\begin{lstlisting}[language=c]
typedef struct __lock_t {
  int flag;
  int guard;
  queue_t *q;
} lock_t;
void lock_init(lock_t *m) {
  m->flag = 0;
  m->guard = 0;
  queue_init(m->q);
}
void lock(lock_t *m) {
  while (TestAndSet(&m->guard, 1) == 1)
     ; //acquire guard lock by spinning
  if (m->flag == 0) {
    m->flag = 1; // lock is acquired
    m->guard = 0;
  } else {
    queue_add(m->q, gettid());
+   setpark();  // avoid wakeup/waiting race
    m->guard = 0;
    park(); // syscall
  }
} // + indicates new code
void unlock(lock_t *m) {
  while (TestAndSet(&m->guard, 1) == 1)
     ; //acquire guard lock by spinning
  if (queue_empty(m->q))// no one wants it
    m->flag = 0;          // let go of lock
  else // hold lock for next thread
    unpark(queue_remove(m->q));
  m->guard = 0;
}
\end{lstlisting}
\end{minipage}
\begin{minipage}{.4\linewidth}
  \flushleft
  \begin{itemize}
  \item this approach doesn't avoid spin-waiting entirely: a spin-lock around \texttt{flag} and \texttt{queue} (lock is using)
  \item \texttt{guard} ensures queue is modified atomically
  \item waiting thread $\to$ queue and \texttt{park()} to sleep
  \item thread releasing lock $\to$ \texttt{unpark()} waiting threads
  \item time spent spinning is quite limited (just a few instructions inside lock and unlock code, instead of user-def critical section)
  \item if thread not acquired lock, adds itself to queue
  \item When a thread woken up, it is as if returning from \texttt{park()}; yet it does not hold \texttt{guard} at that point and thus cannot try to set \texttt{flag} to 1. lock passed directly from releasing-thd to next waiting thd. \texttt{flag} is not set to 0 in-between
  \item \texttt{setpark} makes \texttt{park} return immediately
  \end{itemize}
\end{minipage}
