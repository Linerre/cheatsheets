\documentclass[8pt,a4paper,landscape]{extarticle}
\input{preamble}

\begin{document}
% Suppress page number for all pages
\pagestyle{empty}

% Notes begin
\begin{multicols*}{3}
% \input{subs/ch14_vm_api}
% \input{subs/ch15_vm_addrtrans}
% \input{subs/ch16_vm_seg}
% \input{subs/ch20_ptarithm}

% \pagebreak
% \input{subs/ch18_intro_paging}
% \columnbreak
% \input{subs/ch19_tlb}
% \input{subs/ch20_hybrid}
% \input{subs/ch20_2levelpt}
% \input{subs/ch20_multi_level_pt}
% \input{subs/ch21_swap}
% \columnbreak
% \input{subs/ch22_swap_policy}

\section*{Lock}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c,xrightmargin=2pt]
// globally-allocated lock 'mutex'
lock_t mutex; // declare lock var
// ... other work ...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \begin{itemize}
  \item must declare a lock before using it
  \item lock available = unlocked, free $\to$ no thread holds it
  \item lock acquired = locked, held $\to$ exactly one thread holds it (likely in a critical section)
  \end{itemize}
\end{minipage}
\begin{itemize}
\item other info like which thread holds lock or a queue for ordering lock acquisition can be stored in lock but often hidden from user
\item calling \texttt{lock()} will try to acquire lock (1. free; 2. not free)
  \begin{enumerate}
  \item thread acquires lock and enters critical section; \textbf{owner} of lock
  \item thread waits (not return) and cannot enter critical section
  \end{enumerate}
\item calling \texttt{unlock()} will free the lock (1. no waiting thread; 2. opposite)
  \begin{enumerate}
  \item lock becomes free and any thread can try to acquire it
  \item one of waiting threads will acquire lock and enters critical section
  \end{enumerate}

\item \textbf{coarse-grained}: one big lock for access to critical section at any time
\item \textbf{fine-grained}: different locks to protect different data and data structs (allowing more threads in different locked code at once)
\end{itemize}
\section*{Evaluating Locks}
\begin{enumerate}
\item \textbf{correctness} lock provides \textbf{mutual exclusion}
\item \textbf{fairness} each thread gets a fair shot at acquiring the lock (once it's free); no thread contending for the lock starve while doing so
\item \textbf{performance} time overhead added by using the lock:
  \begin{enumerate}
  \item single thread (no contention) $\to$ what is the overhead?
  \item multiple threads contending for lock on single CPU
  \item lock perf. on multiple CPUs and threads contending for the lock
  \end{enumerate}
\end{enumerate}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c,xrightmargin=2pt]
void lock() { // before entering
  DisableInterrupts();
}
void unlock() { // after leaving
  EnableInterrupts();
}
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \begin{itemize}
  \item one of earliest solution offers mutex by disabling interrupts
  \item code inside critical section (not interrupted) runs as if it were atomic
  \item invented for single-processor systems;  main advantage: simplicity
  \end{itemize}
\end{minipage}
\begin{enumerate}
\item \textbf{insecure} arbitrary thread will need to perform \emph{privileged} ops so greedy/malicious progs can monopolize CPU and exploit system
\item \textbf{importable} not working on multiprocessors: threads enter same critical section via other CPUs $\to$ null effect of disabling interrupts
\item \textbf{buggy} turning off interrupts for too long $\to$ lost useful/critical interrupts that make OS work as expected (e.g. disturb I/O awakening)
\end{enumerate}
Thus, limited use case: OS occasionally uses interrupt masking to guarantee atomicity when accessing its own data structures
\section*{Failed Attempt: Just Using Loads/Stores}
\begin{minipage}{.5\linewidth}
\begin{lstlisting}[language=c,xrightmargin=2pt]
typedef struct __lock_t
{ int flag; } lock_t;

void init(lock_t *mutex) {
  // 0: lock is avail | 1: held
  mutex->flag = 0; }

void lock(lock_t *mutex) {
  while (mutex->flag == 1) // TEST
     ; // spin-wait (do nothing)
  mutex->flag = 1; // now SET it }

void unlock(lock_t *mutex) {
   mutex->flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.5\linewidth}
  \begin{itemize}
  \item use 1 var \texttt{flag} to indicate thread's ownership of lock
  \item when lock not free, other threads \textbf{spin-wait} in while loop
  \item once lock free, waiting thread gets lock $\to$ set flat to 1
  \item correctness problem: both of two threads can set flag to 1 (with interrupts) and enter same critical section $\to$ mutex \textbf{not} guaranteed
  \item performance issue: thread endless checks flag (spin-waiting) and wastes CPU cycles
  \end{itemize}
\end{minipage}
\section*{Working Spin Locks with Test-And-Set (atomic exchange)}
\begin{itemize}
\item test old value and simultaneously setting the mem location to new value
\item return the old value pointed by \texttt{old\_ptr} and set \texttt{new} at same time
\item this sequence of operations is performed \textbf{atomically}
\end{itemize}
\begin{minipage}{.7\linewidth}
\begin{lstlisting}[language=c]
int TestAndSet(int *old_ptr, int new) {
  int old = *old_ptr;// fetch old value at old_ptr
  *old_ptr = new;      // store 'new' into old_ptr
  return old;          // return the old value
}

typedef struct __lock_t { int flag; } lock_t;

// 0: lock is avail, 1: lock is held
void init(lock_t *lock) { lock->flag = 0; }

void lock(lock_t *lock) {
   while (TestAndSet(&lock->flag, 1) == 1)
     ; // spin-wait (do nothing)
}

void unlock(lock_t *lock) { lock->flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.3\linewidth}
  \flushleft
  \begin{itemize}
  \item make both \textbf{test} (old lock value) and \textbf{set} (new value) a single atomic op $\to$ only one thread acquires the lock $\to$ working mutual exclusion primitive
  \item On single CPU, needs a preemptive scheduler (interrupt via timer); otherwise a thread spins on a CPU and never gives it up
  \end{itemize}
\end{minipage}
\begin{enumerate}
\item \textbf{correctness} yes, only a single thread enters critical section at a time
\item \textbf{fairness} not generally, thread may spin forever $\to$ starve/zero fairness
\item \textbf{performance} \textbf{a} on single CPU, pretty bad: if N threads contending for the lock, each thread spins for duration of time slice before being timer-preempted. \textbf{b} on multiple CPU, reasonably well if \# of threads $\approx$ \# of CPUs and critical section is short (wasting fewer CPU cycles)
\end{enumerate}

\section*{Compare-and-Swap (SPARC) or Compare-and-Exchange (x86)}
\begin{minipage}{.65\linewidth}
\begin{lstlisting}[language=c]
// only diff with set-and-test shown below
int CompareAndSwap(int *ptr,
                     int expected, int new)
{
    int original = *ptr;
    if (original == expected)
       *ptr = new;
    return original;
}
void lock(lock_t *lock) {
   while(CompareAndSwap(&lock->flag,0,1) == 1)
      ; // spin
} // more powerful than test-and-set
\end{lstlisting}
\end{minipage}
\begin{minipage}{.35\linewidth}
  \flushleft
  \begin{itemize}
  \item test if value at addr \texttt{ptr} == expected;
  \item if so, update mem location \texttt{ptr} to new.
  \item If not, do nothing.
  \item In either case, return orig val at that mem location, thus allowing the code calling compare-and-swap to know if it succeeded
  \item for lock-free struct
  \end{itemize}
\end{minipage}
a simple spin lock built with it \texttt{==} test-and-set spin lock analyzed above
\section*{Load-Linked + Store-Conditional (MIPS, Alpha, PowerPC, ARM)}
\begin{minipage}{.65\linewidth}
\begin{lstlisting}[language=c]
int LoadLinked(int *ptr) { return *ptr; }
// atomic operation
int StoreConditional(int *ptr, int new) {
  if (no update to *ptr
      since LoadLinked to this address) {
     *ptr = new;
     return 1; // success!
  } else { return 0; }  // failed to update
}
// Using LL/SC To Build A Lock
void lock(lock_t *lock) {
  while (1) {
    while (LoadLinked(&lock->flag) == 1)
       ; // spin until it's zero
    if (StoreConditional(&lock->flag, 1) == 1)
       return;
      // if set-it-to-1 is a success: all done
      // otherwise: try it all over again
    }
}
void unlock(lock_t *lock) { lock->flag = 0; }
\end{lstlisting}
\end{minipage}
\begin{minipage}{.35\linewidth}
  \flushleft
  \begin{itemize}
  \item load-linked operation likes load ix $\to$ fetch a value from addr \texttt{ptr} and place in a register
  \item store-cond succeeds (and updates the val) \emph{only} if no intervening store to the addr has taken place
  \item if succeeded, returns 1; val at \texttt{ptr} $\to$ \texttt{new}
  \item if failed, returns 0; val at \texttt{ptr} \emph{not} updated
  \item suppose 2 threads both exec \texttt{LoadLinked} and proceed to store-cond
  \item at this point only 1 thread will succeed in updating flag to 1
  \end{itemize}
\end{minipage}
\end{multicols*}
\end{document}
